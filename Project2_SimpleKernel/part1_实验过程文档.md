# Project2_SimpleKernel 实验过程文档

## ——Part1

+ 说明：Part1中主要函数的功能、PCB结构、调度和锁机制等见[part1_design_review.md](part1_design_review.md)，本文主要介绍实验过程中遇到的具体问题及其解决措施

### 队列相关API的实现

1. 队列的初始化和维护

   + 通过`LIST_HEAD`宏定义所定义的队列，其`next`和`prev`指针均指向自身，这导致对链表的维护较为困难和不直观，在修改链表中节点时很可能改变队列本身，从而导致指针指飞的情形。
   + 笔者这里选择了一个较为直观的方式，即初始化时令队列头尾指针均为空，在后续维护过程中由`prev`指针指向链表头结点，`next`指针指向链表尾结点。头结点删除和尾结点插入时都能够更方便进行队列的修改和判断。这里应当说明的一点是，该双向链表并非循环链表，因此如果需要将头结点转移至尾部，应当先将其从头部删除，再从尾部插入。

2. 和PCB结合的封装

   + 在进行入队出队操作的时候，更加方便直观的操作是将PCB结构体指针和队列指针作为参数。然而二者的类型不同，因此需要进行相应的封装。
   + 由PCB到链表节点

   ```c
   void enqueue(list_head* queue,pcb_t* pnode){
       list_node_t *lnode = &(pnode->list);
       list_add_tail(queue,lnode);
   }
   ```

   + 由链表节点到PCB

   ```c
   pcb_t * dequeue(list_head* queue){
       //判断非空时才可出队
       if(queue->prev == NULL)
           return NULL;
       
       pcb_t * tmp = list_entry(queue->prev,pcb_t,list);
       list_del_head(queue);
       return tmp;
   }
   ```

   + 说明：从PCB到链表节点较为容易，直接对PCB中链表节点的成员变量`list`进行取地址操作即可。而从链表节点到PCB的操作较为复杂，实质上是根据PCB结构体中`list`成员地址进行一定偏移去获取相应PCB的地址。该方法被封装为一个宏定义`list_entry`，如下所示：

   ```c
   //(type *)0强制转化为地址为0的type类型指针。下述宏定义获得了member成员相对于type指针入口的偏移
   #define offsetof(TYPE, MEMBER) ((size_t) &((TYPE *)0)->MEMBER)
   //第一行将进行类型检查，确定ptr与member类型是否一致。第二行则是根据成员变量减去偏移值得到了容器值。
   #define list_entry(ptr, type, member)                    \
       ({                                                     \
           const typeof(((type *)0)->member) *__mptr = (ptr); \
           (type *)((char *)__mptr - offsetof(type, member)); \
       })
   ```



### init_pcb中栈的分配和维护

+ PCB结构中的内核栈和用户栈会分别通过`allocKernelPage`和`allocUserPage`进行分配。但需要注意的是，查看`mm.c`和`mm.h`可见，这两个函数将会分别在内核和用户可动态的空间中进行栈的分配，而其返回值是所分配的栈的栈底地址。注意到，栈是自顶向下生长的，在`init_pcb_stack`中，我们也是从栈顶地址开始逐个排布栈的内容。因此在进行`PCB`中`kernel_sp`和`user_sp`的初始化时，应当加上所分配的大小以使其达到栈顶。

```c
//alloc返回的是栈底，需要先移动到栈顶再填数据
pcb[process_id].kernel_sp = allocKernelPage(1)+PAGE_SIZE;   
pcb[process_id].user_sp = allocUserPage(1)+PAGE_SIZE;
```



### switch_context_t 结构体的初始化

+ 在task1-2中，用户程序始终运行在内核态。通过第一次`do_scheduler`中的`switch_to`将`ra`和`sp`切换为了用户程序对应的，由此开始执行。因此在初始化`switch_context_t`结构体时，应当将`ra`初始化为用户程序入口地址，以使`switch_to`将返回用户程序，应当将`sp`初始化为`kernel_sp`，即已排布内容的底部，以使得运行在内核态的用户程序可以利用内核栈剩余的空白空间，而不会修改已排布的结构体内容。
+ 需要注意的是，在part2时，用户程序切换为`syscall`版本，将会运行到用户态，第一次启动的逻辑也有所不同。`switch_to`需要先跳转到`ret_from_exception`，通过恢复上下文和切换到用户态，跳转到用户程序也是由于`sret`返回`epc`位置而非仍通过`ra`恢复。此时`switch_to`结构体中所存储的`ra`也应当相应的更改为`ret_from_exception`函数的入口地址。



### swith_to中sp的维护

+ 在代码实现中使用`t0`作为中间寄存器存储新旧进程的`switchto_context_t`结构体指针，通过相对其偏移的形式进行寄存器的保存和恢复。但在刚开始运行时，发现初末位置对sp的更改导致了报错，这是因为结尾处`sp`的增加使得sp指向了`switchto_context_t`的顶部，跳转到用户程序时，将会修改结构体内容造成错误。
+ 依照原有代码中将SP寄存器先减后加的操作，另一种实现方法是通过sp作为中间寄存器，进行`switch_context_t`结构体存取。该方案会在当前栈中存储当前进程寄存器，并更改`kernel_sp`指针。在新进程取寄存器值时，由于涉及sp改动，应当调整sp取值顺序至最后。最后将sp增大，一方面为了保持一致性，另一方面可以避免栈错误使用时更改结构体内容。



### 单锁机制下的正确现象和错误代码

+ 单锁机制的第一版代码中，遗漏了锁释放函数中对锁状态的改变，也忽略了在申请锁时需要处于循环中，直到成功申请到锁才可退出循环。
+ 上述代码造成了以下情形：
  1. 当A进程上锁之后，B进程申请锁失败，进入阻塞队列。
  2. A进程释放锁时，解除B进程阻塞并调度执行，由于B进程未处于循环中，因此直接开始执行其操作，由于A进程忘记修改锁状态，看起来就像B进程重新申请锁并修改状态。
  3. B进程执行时，A进程堵塞。当B进程释放锁时，A进程调度执行，仍是未申请锁就直接开始运行，由于上锁状态未修改，看起来就像A进程重新申请了锁。
+ 这一情形看似与预期现象是一致的，但是并未进行锁状态的修改和再上锁，和锁的概念相违背。在多锁机制引入新的锁测试程序时，也会发现了不同程序申请到了同一把锁，这无疑是错误的。
+ 正确的代码修改方式是在释放锁函数中需要修改锁状态，且申请锁应当循环获取。只有获取到锁才允许结束循环开始执行后续程序。



### 多锁机制下的阻塞和释放机制

+ 引入多锁机制后不能简单进行堵塞队列的清空，因为其中的任务可能还需要其他锁。此时应当考虑当某些锁被占用时，应当将该任务入队哪些阻塞队列。
+ 考虑一下情形，A任务已占用锁1、2；B任务申请锁2、3，堵塞；C任务占用锁3、4。然后A和C任务先后释放锁。在该情形下，一种可能的阻塞方式是B先进入锁2的阻塞队列，当A任务释放时，考虑B需申请锁的情况，发现锁3仍被占用，于是进入锁3的阻塞队列。这种方式维护较为复杂，特别是当某任务的锁与多个其他任务重合时，就会频繁的在不同所的阻塞队列中转移。
+ 一种维护较为简单的，效果与其等效的方式是，当该任务申请时，若某些锁被占用，就将其加入所有申请锁的阻塞队列。该方式可以省去在不同队列间转移的开销，当某锁被释放时，只需对其中任务检查其他锁的占用情况，以决定重新入队该阻塞队列，或者释放即可。使用该方式进行堵塞时，需要额外在释放时考虑自身仍在所释放的阻塞队列的情况，将自身出队即可。
+ 锁释放的相应实现细节可见[part1_design_review.md](part1_design_review.md)多锁机制相关部分。



### 多锁机制下的进程饥饿现象及解决

+ 多锁机制下不同任务的相关锁序列只是部分重合时，可能由于block_queue进入ready_queue顺序产生进程饥饿。例如，lock1和mylock占用锁1、2，lock2占用锁2、3。当lock1占用，将另外二者堵塞，由于先遍历锁1阻塞度列，mylock先释放。mylock执行时，将另外二者堵塞，lock1先释放。上述过程循环进行，导致lock2无法获取锁，进程饥饿。
+ 一种可行的解决方式是使用ROB(Re-order Buffer)重排缓冲区进行解决。在PCB结构体中增加`lock_time`统计每个任务获取锁的次数。在释放锁遍历阻塞队列时，暂缓将任务`do_unblock`入队`ready_queue`，而是先将其排序，获取锁次数越少，优先级越高。再将重排后的ROB依次入队。
+ 该方法可以实现锁在各进程间的公平分配，解决进程饥饿问题。

